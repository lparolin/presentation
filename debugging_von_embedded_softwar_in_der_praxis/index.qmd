---
title: "Debugging von Embedded Software in der Praxis"
author: "Dr. Luca Parolini"
format:
  revealjs:
    theme: [default, styles.scss]
    html: true
    fig-align: left
    slide-number: true
    auto-animate: true
    callout-icon: false
    title-slide-attributes:
      data-background-image: "Figuren/First_Computer_Bug_1945.jpg" # Path to your image
      data-background-size: cover # Ensures full coverage
      data-background-position: center # Optional: centers image
      data-background-opacity: "0.3"
      data-background-color: "black"

preview:
  watch-inputs: true
---

## Von Bugs zu Debugging {auto-animate=true}

### Historischer Überblick

Der Begriff _Bug_ für technische Fehler lässt sich bis ins 19. Jahrhundert zurückverfolgen.

::: {.incremental}

- 1878: Thomas Edison beschreibt „kleine Fehler“ in seinen Erfindungen als _Bugs_.[^edison]
- 1947: Mark II Aiken Relay Calculator – berühmter „Bug“ (Motte im Relais).[^mark2]

:::

:::{.fragment .fade-left}
:::{.columns}
:::{.column width="60%"}
![](Figuren/First_Computer_Bug_1947_smaller.png){fig-alt="Motte im Relais" width="100%"}
:::
:::{.column width="40%"}
Motte zwischen den Kontakten am Relais Nr. 70, Panel F, des Mark II Aiken Relay Calculator (Harvard University, 1947).[^motte]

:::
:::
:::

[^edison]: Thomas Edison, Brief an William Orton, Präsident von Western Union, 13. März 1878.
[^mark2]: Grace Hopper, _The First Bug_, 1986.
[^motte]: Courtesy of the Naval Surface Warfare Center, Dahlgren, VA., 1988., Public Domain. [Link](https://de.wikipedia.org/wiki/Debuggen#/media/Datei:First_Computer_Bug,_1945.jpg)

## Von Bugs zu Debugging {auto-animate=true}

### Debugging ist nicht gleicht Troubleshooting

Der Begriff _Bug_ für technische Fehler lässt sich bis ins 19. Jahrhundert zurückverfolgen.

- 1878: Thomas Edison beschreibt „kleine Fehler“ in seinen Erfindungen als _Bugs_.[^edison]
- 1947: Mark II Aiken Relay Calculator – berühmter „Bug“ (Motte im Relais).[^mark2]

:::{.spacer-lg}
:::

::: {.callout-note appearance="simple" .fade-left}

> _Debugging bedeutet in der Regel herauszufinden, warum ein Entwurf nicht wie geplant funktioniert. Troubleshooting hingegen bedeutet herauszufinden, was an einer bestimmten Instanz eines Produkts defekt ist, wenn das Produktdesign an sich als korrekt gilt._ (nach Agans, 2002)[^agans]

<!-- ::: {.small-text}

> Debugging usually means figuring out why a design doesn't work as planned. Troubleshooting usually means figuring out what's broken in a particular copy of a product when the product's design is known to be good.(Agans, 2002)[^agans]

::: -->

:::

[^edison]: Thomas Edison, Brief an William Orton, Präsident von Western Union, 13. März 1878.
[^mark2]: Grace Hopper, _The First Bug_, 1986.
[^agans]: David, J. Agans. "The 9 indispensable rules for finding even the most elusive software and hardware problems." (2002).

## Vielfältigkeit eingebetteter Systeme

### von Sensoren bis zu Triebwerken

Eingebettete Systeme sind in ihrer Architektur, Funktionalität und Anwendung stark unterschiedlich.

- Standards und Vorschriften variieren deutlich je nach Branche
- Sicherheits- und Datenschutzanforderungen benötigen zusätzliche Entwicklung- und Testprozesse

:::{.columns}
:::{.column width="50%"}

![](Figuren/BeispieleEmbeddedSystems.png){fig-alt="Beispiele eingebetteter Systeme" width="100%"}
:::

:::{.column width="50%" .incremental }

Einige Gemeinsamkeiten bestehen jedoch:

- Ressourcenbeschränkungen (Speicher, Rechenleistung)
- Echtzeitanforderungen
- Die gesamte Entwicklung erfordert **Zusammenarbeit zwischen verschiedenen Teams** und Disziplinen
- **Besondere Schwierigkeit beim SW Debugging**
  - Eingeschränkte Beobachtbarkeit von internen Zuständen
  - Fehler treten oft nur unter spezifischen Echtzeitbedingungen auf
  - Hardwarezugriff ist begrenzt oder erfordert Spezialwerkzeuge

:::
:::

::: {.slide-footnote}
Abbildung ist mit Adobe Firefly generiert.
:::

## Reproduzierbarkeit von Defekten

Ein zentrales Thema beim Debugging eingebetteter Software ist die _Reproduzierbarkeit_ von Fehlern.

::: {.mid-space}

- Effektives Debugging setzt voraus, dass ein Defekt **wiederholt ausgelöst** und sichtbar gemacht werden kann.

  - Von entscheidender Wichtigkeit: Fehler müssen immer **stimuliert** werden und niemals nur simuliert.

::: {.incremental}

- Vorteile der Reproduzierbarkeit:
  - Fehler lässt sich wieder sichtbar machen
  - Ursachenanalyse wird möglich
  - Fix kann überprüft werden

:::
:::

::: {.fragment}

In eingebetteten Systemen ist dies jedoch häufig schwierig wenn nicht unmöglich

- Fast immer sind mehrere Einflussfaktoren (z. B. zeitliche Randbedingungen, Umgebungsbedingungen) nicht unter unserer Kontrolle und oft auch nicht zuverlässig messbar.

:::

::: {.spacer-md}
:::

::: {.fragment}
:::{.callout-note appearance="simple"}

**Ohne Reproduzierbarkeit ist Software‑Debugging praktisch unmöglich.**
:::
:::

# Technische und prozessuale Methoden {background-image="Figuren/First_Computer_Bug_1945.jpg" background-opacity="0.3" background-color="black"}

## Instrumentierung und Tracing für Timing‑Analyse {auto-animate=true}

### Beispiel: Elektromotorregler

Wir haben einen Regler für einen Elektromotor entwickelt.

- Die Reglerfunktion ist jede **15 ms** aufgerufen und hat ein maximales Timing‑Budget von **2 ms**.
- Wir möchten überprüfen wie lange unsere Funktion wirklich braucht um alle die Berechnungen abzuschließen.

::: {.columns}
:::{.column width="60%"}
:::{.fragment}
**Naiver Ansatz: printf**

```c{code-line-numbers="|3,6"}
bool applyControlRule(const InputParameter* motor_state)
{
    printf("applyControlRule - Start: %ld ms\n", get_time_ms());
    bool is_rule_successfully_applied = true;
    // ... hier sind die eigentliche Regelung aufgerufen ...
    printf("applyControlRule - End: %ld ms\n", get_time_ms());
    return is_rule_successfully_applied;
}
```

:::

:::

:::{.column width="40%"}

:::{.fragment}
**Ausgabe:**

```shell
applyControlRule - Start: 1000 ms
applyControlRule - End: 1006 ms
...
applyControlRule - Start: 1015 ms
applyControlRule - End: 1023 ms
...
```

::: {.small-text}

- Die gemessene Gesamtdauer beträgt ca. **6 ms**
- **Der Wert ist jedoch nicht korrekt!**

:::
:::
:::
:::

::: {.fragment}

Jeder Aufruf von printf kann mehrere Millisekunden beanspruchen

- Das **Zeitverhalten** wird dadurch erheblich beeinträchtigt und Deadlines können verfehlt werden.

:::

::: {.fragment}
:::{.callout-note appearance="simple"}
**Niemals printf im Echtzeitpfad verwenden!**
:::
:::

## Instrumentierung und Tracing für Timing‑Analyse {auto-animate=true}

### Beispiel: Elektromotorregler

::: {.columns}
:::{.column width="60%"}

**Besser Ansatz: Pin-Toggling**

```c{code-line-numbers="3,6"}
bool applyControlRule(const InputParameter* motor_state)
{
    gpio_set(GPIO_PIN_DEBUG, true);
    bool is_rule_successfully_applied = true;
    // ... hier sind die eigentliche Regelung aufgerufen ...
    gpio_set(GPIO_PIN_DEBUG, false);
    return is_rule_successfully_applied;
}
```

:::

:::{.column width="40%"}
Messungen:
![](Figuren/Timing_Diagram.svg){width=100%}

::: {.small-text}
Die gemessene Gesamtdauer beträgt jetzt ca. **1,8 ms** und ist sie somit innerhalb das maximal Time-Budget von 2 ms.
:::

:::
:::

:::{.spacer-sm}
:::

::: {.fragment}
Diese Art von Zeitliche Messungen ermöglicht, in vergleicht zu printf Ansatz:

- **Deutlich höhere Genauigkeit** der Laufzeitmessung
- **Reproduzierbarkeit** der Ergebnisse über viele Zyklen hinweg
- **Minimale Beeinflussung** der Echtzeit (Overhead im Nanosekunden-/Mikrosekundenbereich)

:::

## Instrumentierung und Tracing für Timing‑Analyse {auto-animate=true}

### Beispiel: Elektromotorregler

::: {.columns}
:::{.column width="60%"}

**Besser Ansatz: Pin-Toggling**

```c{code-line-numbers="3,6"}
bool applyControlRule(const InputParameter* motor_state)
{
    gpio_set(GPIO_PIN_DEBUG, true);
    bool is_rule_successfully_applied = true;
    // ... hier sind die eigentliche Regelung aufgerufen ...
    gpio_set(GPIO_PIN_DEBUG, false);
    return is_rule_successfully_applied;
}
```

:::

:::{.column width="40%"}
Messungen:
![](Figuren/Timing_Diagram.svg){width=100%}

::: {.small-text}
Die gemessene Gesamtdauer beträgt jetzt ca. **1,8 ms** und ist sie somit innerhalb das maximal Time-Budget von 2 ms.
:::

:::
:::

:::{.spacer-sm}
:::

Der Ansatz hat aber auch Einschränkungen:

- **Begrenzte Ausgabe**: nur wenige Informationen können über Pins dargestellt werden

- Vorgeschlagene Alternative: **Logger und Trace‑Tools**
  - Ermöglichen strukturierte, präzise und echtzeitfähige Messungen.

## Präventive Maßnahmen in der Embedded‑Software‑Entwicklung {auto-animate=true}

### Ziel ist es, Fehler frühzeitig zu entdecken

Präventivemaßnahmen müssen in den Entwicklungsprozess von Software eingebaut werden, z.B.:

- Prozessmaßnahmen:

  ::: {.fragment}

  - Code Reviews / Peer Reviews
  - Testautomatisierung (Continuous Integration / Continuous Deployment - CI/CD)
  - Test-Driven Development (TDD)

  :::

- Technische Maßnahmen:

  ::: {.fragment}

  - Statische Code-Analyse
  - Code‑Coverage‑Analyse (z. B. Line‑ und Branch‑Coverage)
  - Design by Contract (Vertragsbasierte Programmierung)

  :::

::: {.fragment}

**Eine Regelfunktion für die Warmwasserbereitung einer Kaffeemaschine**

```cpp
uint16_t computeHeatingTime(const uint8_t initialTemp,
                            const uint8_t finalTemp)
{
    const uint8_t conversion_factor = 1; // seconds per degree Celsius
    const uint8_t deltaT = finalTemp - initialTemp;
    const uint16_t time_sec = conversion_factor * deltaT;
    return time_sec;
}
```

:::

:::{.notes}

- Prozessmaßnahmen:
  - Code Reviews / Peer Reviews
  - Testautomatisierung (Continuous Integration / Continuous Deployment - CI/CD)
  - Test-Driven Development (TDD)
- Technische Maßnahmen:
  - Statische Code-Analyse
  - Code‑Coverage‑Analyse (z. B. Line‑ und Branch‑Coverage)
  - Design by Contract (Vertragsbasierte Programmierung)

:::

## Präventive Maßnahmen in der Embedded‑Software‑Entwicklung {auto-animate=true}

### Ziel ist es, Fehler frühzeitig zu entdecken

**Eine Regelfunktion für die Warmwasserbereitung einer Kaffeemaschine**

```{.cpp code-line-numbers="5"}
uint16_t computeHeatingTime(const uint8_t initialTemp,
                            const uint8_t finalTemp)
{
    const uint8_t conversion_factor = 1; // seconds per degree Celsius
    const uint8_t deltaT = finalTemp - initialTemp;
    const uint16_t time_sec = conversion_factor * deltaT;
    return time_sec;
}
```

Was passiert, wenn `initalTemp` großer als `finalTemp` ist?

:::{.fragment}

- Bei `finalTemp=95` und `initialTemp=96` ergibt sich **time_sec=255**
- Das entspricht **4 Minuten und 15 Sekunden!**

:::

:::{.spacer-md}
:::

::: {.callout-note appearance="simple" .fragment}
**Solche Fehler müssen bereits in der Entwicklungsphase entdeckt werden!**
:::

::: {.notes}
Weitere Präventivmaßnahmen
Code Reviews / Peer Reviews → Frühes Auffinden von Fehlern durch das Vier‑Augen‑Prinzip; fördert auch Wissensaustausch im Team.

Pair Programming → Zwei Entwickler arbeiten gleichzeitig am selben Code; reduziert Tippfehler und Denkfehler.

Formale Methoden / Modellprüfung → Mathematische Verifikation von Algorithmen und Zustandsautomaten; besonders relevant in sicherheitskritischen Systemen.

Defensive Programmierung → Eingaben validieren, Fehlerfälle explizit behandeln, Assertions einsetzen → verhindert „stille“ Fehler.

Coding Standards (MISRA‑C, CERT‑C, AUTOSAR) → Einheitliche Regeln für Sprache und Stil; vermeiden typische Fehlerquellen in Embedded Systems.

Continuous Monitoring / Logging → Laufzeitüberwachung und strukturierte Logs helfen, Fehler früh zu erkennen und reproduzierbar zu machen.

Simulation & Hardware‑in‑the‑Loop Tests (HIL) → Testen mit realistischen Umgebungen, bevor Hardware verfügbar ist; reduziert Überraschungen beim Integrations‑Debugging.

Fuzz Testing → Zufällige oder fehlerhafte Eingaben automatisch generieren, um Robustheit zu prüfen.
:::

## Das V‑Modell {auto-animate=true}

### Anforderungen, Tests und Kommunikationsrisiken zwischen Teams

- Das V‑Modell entstand Ende der 1970er Jahre als Erweiterung des Wasserfallmodells,
  vorgeschlagen von Barry W. Boehm (1979), um Verifikation und Validierung systematisch zu verankern.

::: {.r-stack}
![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell_Teil1.svg){.fragment width="100%"}

![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell_Teil2.svg){.fragment width="100%"}

![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell.svg){.fragment width="100%"}

:::

::: {.slide-footnote}
Boehm, B. W. (1979). _Guidelines for Verifying and Validating Software Requirements and Design Specifications._ In Proceedings of the European Conference on Applied Software Engineering
:::

:::{.notes}

Barry W. Boehm

- Geboren 1935, gestorben 2022.
- Professor an der University of Southern California (USC).
- Arbeitete zuvor u. a. bei RAND Corporation und TRW.
- Bekannt für grundlegende Beiträge zur Softwareentwicklung, u. a.:
- Spiralmodell (1986) – ein iteratives Vorgehensmodell.
- COCOMO‑Modell (Constructive Cost Model) – zur Aufwandsschätzung von Softwareprojekten.
- V‑Modell (1979) – als Erweiterung des Wasserfallmodells.

:::

## Das V‑Modell {auto-animate=true}

### Anforderungen, Tests und Kommunikationsrisiken zwischen Teams

::: {.spacer-sm}
:::

::: {.columns}
:::{.column width="45%"}
![](Figuren/V_modell.svg){fig-alt="V-Model without axes" width="100%"}
:::

::: {.column width="55%" .small-text}
::: {.spacer-sm}
:::

- Anforderungen werden iterativ verfeinert, Testfälle daraus abgeleitet
- Anforderungen definieren das **Soll‑Verhalten**, Testfälle prüfen es
- In der Testphase ist das Soll‑Verhalten ausschließlich durch die Testfälle beschrieben

::: {.spacer-sm}
:::

::: {.fragment fragment-index=2}

- **Unterschiedliche Teams** arbeiten häufig auf verschiedenen Ebenen des V‑Modells
- Entwurfsteams sind typischerweise nicht identisch mit den Teams, die Tests und Verifikation durchführen
- **Kommunikationsfehler** können dazu führen, dass Anforderungen oder Testfälle falsch abgeleitet werden
- Solche Fehler werden oft erst in der Testphase erkannt

:::
:::
:::

:::{.spacer-sm}
:::

::: {.fragment}

- Software‑Debugging erfolgt häufig als Reaktion auf einen Defekt.
- Defekte entstehen in der Test- und Validierungsphase wenn das **Ist-Verhalten** nicht das **Soll-Verhalten** entspricht.

:::

## Kommunikationsfehler als Defektursache {auto-animate=true}

Wir wollen ein Spurhaltewarnsystem (_Lane departure warning system_) (LDWS) entwickeln und die vordere Kamera ist ein Kernsensor.

::: {.fragment}

- **Anforderung P.1:** LDWS muss aktive sein, wenn die Kamera einen nominalen Zustand meldet (keine Degradation).
- **Anforderung P.2:** LDWS muss sich sicher deaktivieren, wenn die Kamera eine Degradation meldet.

::: {.fragment}

- ...
  - ↳ **Kamera-Anforderung K.1:** Die Kamera muss eine Degradation melden, wenn ungeeignete Wetterbedingungen erkannt werden.
    - Beispiele: Regentropfen auf den Linsenelementen, Blendung durch Sonnenlicht

:::

:::

::: {.slide-footnote}
Alle Anforderungen und Testfälle sind fiktiv und dienen nur zu Illustrationszwecken.
:::

## Kommunikationsfehler als Defektursache {auto-animate=true}

- **Anforderung P.1:** LDWS muss aktive sein, wenn die Kamera einen nominalen Zustand meldet (keine Degradation).
- [**Anforderung P.2:** LDWS muss sich sicher deaktivieren, wenn die Kamera eine Degradation meldet.]{style="color: dimgray;"}
- ...
  - [↳ **Kamera-Anforderung K.1:** Die Kamera muss eine Degradation melden, wenn ungeeignete Wetterbedingungen erkannt werden.]{style="color: dimgray;"}
    - [Beispiele: Regentropfen auf den Linsenelementen, Blendung durch Sonnenlicht]{style="color: dimgray;"}

Im Laufe einer späteren Testphase, wird so ein Testreport herstellt:

::: {data-id="test-fall-example" .small-text}

| Test ID      | Vorbedingungen                            | Soll-Verhalten | Ist-Verhalten                                                                                           | SW-Version | HW-Version |
| ------------ | ----------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------- | ---------- | ---------- |
| **P.1 - 01** | _Ideales Wetter_: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert jedoch sich in eine Stausituation. → **LDWS nicht aktiv!** | 1.1        | C          |

:::

::: {.slide-footnote}
Alle Anforderungen und Testfälle sind fiktiv und dienen nur zu Illustrationszwecken.
:::

## Kommunikationsfehler als Defektursache {auto-animate=true}

::: {data-id="test-fall-example" .tiny-text}

| Test ID      | Vorbedingungen                          | Soll-Verhalten | Ist-Verhalten                                                                                           | SW-Version | HW-Version |
| ------------ | --------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------- | ---------- | ---------- |
| **P.1 - 01** | Ideales Wetter: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert jedoch sich in eine Stausituation. → **LDWS nicht aktiv!** | 1.1        | C          |

:::

::: {.spacer-sm}
:::

::: {.columns}
:::{.column width="30%"}
![Fahrerperspektive](Figuren/Stau_Fahrerperspective.png){fig-alt="Fahrerperspektive: ideale Wetterbedingungen" width="70%"}

:::

:::{.column width="30%" .fragment}
![Kameraperspektive](Figuren/Stau_Kameraperspektive.png){fig-alt="Kameraperspektive: Blendung durch Sonnenlicht" width="70%"}

:::

:::{.column width="40%" .fragment .mid-space}

- Für die Kamera sind diese Wetterbedingungen _nicht_ geeignet, auch wenn von der Fahrerperspektive, Wetterbedienung ideal sind.

- **Kommunikationsfehler:** Die Anforderungen waren korrekt spezifiziert. Die Testbedingungen waren jedoch nicht konsistent mit den Anforderungen und führten zu einem fehlerhaften Testfall.

::: {.spacer-sm}
:::

:::

:::

::: {.callout-note appearance="simple" .small-text .fragment}
Erster Schritt bei der Analyse eines Defekts ist die Überprüfung, ob tatsächlich ein Defekt vorliegt.
:::

::: {.slide-footnote}
Alle Figuren sind mit Microsoft Copilot generiert.
:::

# Otherstuff

## Find a title here

Debugging ist daher ein notwendiger Schritt, um Tests zu validieren, Defekte zu verstehen und das System nachhaltig zu verbessern.“

- Debugging vs. Testing – Unterschied und Rolle im Entwicklungsprozess
- Kosten von Fehlern – warum Prävention wichtig ist
- Verifikation des Defekts – Testreport und Reproduzierbarkeit
- System verstehen – HW, SW, Timing als mögliche Ursachen
- Debugging-Werkzeuge
  - JTAG, UART
  - Trace & Logging (DLT, RTEdbg, SystemView)
  - Debug-Build vs. Optimized Build
- Timing-Probleme – die schwierigsten Fehler
- Prävention statt Heilung
  - Test Driven Development (TDD)
  - Code Coverage
  - Static Code Analysis
- Fehlerbehebung & Absicherung – Regressionstests, Anforderungen ergänzen
- Dokumentation & Lessons Learned

## Debugging und Kommunikationsfehler in Embedded Software

### Anzahl der beteiligten Gruppen erschwert die Defektanalyse

:::{.spacer-sm}
:::

::: {.columns}
:::{.column width="33%"}
![**Thermostat (Smart Home)**](Figuren/Thermostat_square.png){fig-alt="Heizkörper-Thermostat" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** <3
- **Komplexität:** Niedrig

:::
:::

:::{.column width="33%"}
![**Vollautomatik Kaffeemaschine**](Figuren/KaffeMachine_square.png){fig-alt="Vollautomatik Kaffeemaschine" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** 5-10
- **Komplexität:** Mittel

:::
:::

:::{.column width="33%"}
![**Fahrassistenzsysteme**](Figuren/Auto_square.png){fig-alt="Fahrerassistenzsysteme" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** >20
- **Komplexität:** Hoch

:::
:::

:::

::: {.fragment .callout-note appearance="simple" .small-text}
**Risiko von Kommunikationsfehler wächst mit der Zahl der beteiligten Gruppen.**
:::

::: {.slide-footnote}
Alle Figuren sind mit Microsoft Copilot generiert.
:::

## Die Soll-Funktion ergibt sich aus den Anforderungen

### Die Testfällen beschrieben aber sie explicit

::: {.columns}
:::{.column width="70%"}

<!-- ![**Das V-Modell als Entwicklungsmodell**](Figuren/V-Modell.png){fig-alt="V-Model without axes" width="100%"} -->

:::

::: {.column width="30%" .mid-space}

- Jede Ebene ableitet detailliertere Anforderungen für die nächste Ebene sowie passende _Testfälle_.
- Anforderungen beschrieben das Soll-Verhalten für mehrere Input Bedingungen.
- Während eine Testphase, ist die Soll-Verhalten nur anhand von einem Test case für das _System Unter Test_ (SUT) spezifiziert.
- Test cases spezifizieren das Soll-Verhalten für ein spezifische, sehr detaillierten Fall

:::
:::

::: {.callout-note appearance="simple" .small-text}
Fehler können sowohl in der Spezifikation der Anforderungen als auch in der Ableitung der Testfälle entstehen.
:::

::: {.slide-footnote}
Bild von Michael Pätzold, S. Seyfert, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/de/).
:::

## Add me

- Jede System muss sein Input monitorieren und off-line analysieren können
- Logging und Tracing sind kritisch
- Werkzeuge und Techniken um das System Verhalten zu beobachten
- Beispiel: Monitoring von Kaffee Temperatur in einer Kaffeemaschine
- Wie können wir das Verhalten beobachten ohne das System zu beeinflussen?
  - Nie printf befehle in Embedded Systems benutzen!!
  - Log Dateien mit Zeitstempel und Kontextinformationen
  - Trace Analyse Werkzeuge
  - Autmotive ist DLT (Diagnostic Log and Trace) Standard
- Hardware Debugging Werkzeuge
  - JTAG, SWD, etc..
  - Logic Analyzer
  - In-Circuit Emulatoren
- Software Debugging Werkzeuge
  - GDB, LLDB, etc..
  - IDE integrierte Debugger
  - Spezielle Debugging Werkzeuge für Embedded Systems
- Debugging Techniken
  - Breakpoints und Watchpoints
  - Step-by-Step Ausführung
  - Speicher- und Register-Inspektion
  - Code Coverage Analyse
  - Performance Profiling
  - Timing Analyse
  - Kommunikationsprotokoll Analyse
- Zusammenarbeit im Team

  - Gemeinsame Debugging Sitzungen

  - Nie ein Fehler simulieren, Die Fehler müssen stimuliert werden!!
  - Problem: Nicht alle Fehler sind reproduzierbar da nicht alle Input Bedingungen kontrollierbar sind und Status Informationen fehlen können
  - Beispiel: Intermittierende Fehler durch elektromagnetische Störungen
  - Debugging ist ein iterativer Prozess
  - Hypothese generieren, testen, verfeinern
  - Geduld und Ausdauer sind entscheidend

## Defekt Bestätigt: Wie man Root-cause findet

## Temp

```{dot}
digraph Defektanalyse {
    rankdir=TB;
    node [shape=rectangle, fontname="Helvetica"];

    // Start- und Endknoten
    start [shape=circle, style=filled, fillcolor=black, width=0.2, label=""];
    end   [shape=circle, style=filled, fillcolor=black, peripheries=2, width=0.25, label=""];

    // Entscheidung
    start -> A;
    A [label="Defekt entdeckt"];
    A -> B;
    B [shape=diamond, label="Defekt besteht?"];

    // Linker Pfad (Nein) als vertikale Spalte
    subgraph cluster_left {
        rankdir=TB;
        style=invis;
        C [label="Testfall/Konzept prüfen"];
        D [label="Dokumentation"];
        C -> D -> end;
    }
    B -> C [label="Nein"];

    // Rechter Pfad (Ja) als vertikale Spalte
    subgraph cluster_right {
        rankdir=TB;
        style=invis;
        E [label="System verstehen"];
        F [label="Fehler stimulieren"];
        G [label="System instrumentieren"];
        H [label="SW-Änderung identifizieren"];
        I [label="System erneut testen"];
        J [label="Dokumentation"];
        E -> F -> G -> H -> I -> J -> end;
    }
    B -> E [label="Ja"];
}
```

## Debugging in Embedded Systems: Wichtige Überlegungen

```

# Backup

## Wenn man in die Tiefe des Mikrocontrollers schauen muss {auto-animate=true}

Manchmal ist es notwendig, Inhalte von Registern oder Speicher eines Mikrocontrollers auszulesen oder gezielt zu verändern.

- Viele moderne **32‑Bit‑Mikrocontroller** besitzen dedizierte **On‑Chip Debug‑Interfaces** (z. B. JTAG oder SWD)
  - Ermöglicht direkten Zugriff auf Register, Speicher und CPU‑Kontrolle
  <!-- - Typische Werkzeuge:
  - **JTAG (Joint Test Action Group)**^[IEEE 1149.1 – Standard Test Access Port and Boundary-Scan Architecture (IEEE, 2013)]
  - **SWD (Serial Wire Debug)** – abgespeckte Variante für ARM Cortex‑M
  - **OpenOCD (Open On‑Chip Debugger)** + **GDB (GNU Debugger)** im Open‑Source‑Umfeld
  - Kommerzielle Alternativen: **Segger J‑Link**, **Lauterbach TRACE32**, Hersteller‑IDEs (z. B. STM32CubeIDE, NXP MCUXpresso) -->
- On‑Chip Debug-Interfaces erlauben Daten auszulesen, Speicher und Register zu verändern und der Programmfluss zu beeinflussen
  - Breakpoints erlauben es, den Code gezielt anzuhalten und zu überwachen

:::{.columns}
:::{.column width="50%"}
![Verbindung über JTAG^[Bildquelle: Erich Styger, „FTDI JTAG Connection“, MCU on Eclipse Blog, 2019. URL: [link](https://mcuoneclipse.com/wp-content/uploads/2019/10/ftdi-jtag-connection.png)]](Figuren/ftdi-jtag-connection_small.png){width="100%"}
:::

:::{.column width="50%"}
![Debugging mit GDB^[Bildquelle: Espressif Systems, „VS Code ESP-IDF Extension – GDB Commands“, offizielle Dokumentation. URL: [link](https://docs.espressif.com/projects/vscode-esp-idf-extension/en/latest/_images/gdb_commands.png)]](Figuren/gdb_commands_small.png){width="100%"}
:::

:::

## Wenn man in die Tiefe des Mikrocontrollers schauen muss {auto-animate=true}

Manchmal ist es notwendig, Inhalte von Registern oder Speicher eines Mikrocontrollers auszulesen oder gezielt zu verändern.

:::{.mid-space}

- Viele moderne **32‑Bit‑Mikrocontroller** besitzen dedizierte **On‑Chip Debug‑Interfaces** (z. B. JTAG oder SWD)
  - Ermöglicht direkten Zugriff auf Register, Speicher und CPU‑Kontrolle
  <!-- - Typische Werkzeuge:
  - **JTAG (Joint Test Action Group)**^[IEEE 1149.1 – Standard Test Access Port and Boundary-Scan Architecture (IEEE, 2013)]
  - **SWD (Serial Wire Debug)** – abgespeckte Variante für ARM Cortex‑M
  - **OpenOCD (Open On‑Chip Debugger)** + **GDB (GNU Debugger)** im Open‑Source‑Umfeld
  - Kommerzielle Alternativen: **Segger J‑Link**, **Lauterbach TRACE32**, Hersteller‑IDEs (z. B. STM32CubeIDE, NXP MCUXpresso) -->
- On‑Chip Debug-Interfaces erlauben Daten auszulesen, Speicher und Register zu verändern und der Programmfluss zu beeinflussen
  - Breakpoints erlauben es, den Code gezielt anzuhalten und zu überwachen

:::

::: {.space-xl}
:::

::: {.space-xl}
:::
:::{.mid-space .incremental}

- Debugging erfordert oft die Verwendung spezieller **Debug-Flags**
  - Diese deaktivieren Optimierungen und verändern Ablauf sowie Performance gegenüber optimierten Build

:::

::: {.space-lg}
:::

::: {.space-lg}
:::

:::{.fragment}

:::{.callout-note appearance="simple"}

**Zeitliche Probleme bleiben im Debug‑Build häufig verborgen**, da die reale Code-Performance im optimierten Build abweicht.

:::
:::
```
