---
title: "Debugging von Embedded Software in der Praxis"
author: "Dr. Luca Parolini"
format:
  revealjs:
    theme: [default, styles.scss]
    html: true
    fig-align: left
    slide-number: true
    auto-animate: true
    callout-icon: false
    title-slide-attributes:
      data-background-image: "Figuren/First_Computer_Bug_1945.jpg" # Path to your image
      data-background-size: cover # Ensures full coverage
      data-background-position: center # Optional: centers image
      data-background-opacity: "0.3"
      data-background-color: "black"
    footer: "THD · Debugging von Embedded Software in der Praxis · Dr. Luca Parolini"

preview:
  watch-inputs: true
---

## Von Bugs zu Debugging {auto-animate=true}

### Historischer Überblick

Der Begriff _Bug_ für technische Fehler lässt sich bis ins 19. Jahrhundert zurückverfolgen.

::: {.incremental}

- 1878: Thomas Edison beschreibt „kleine Fehler“ in seinen Erfindungen als _Bugs_.[^edison]
- 1947: Mark II Aiken Relay Calculator – berühmter „Bug“ (Motte im Relais).[^mark2]

:::

:::{.columns}
:::{.column width="60%"}
![](Figuren/First_Computer_Bug_1947_smaller.png){fig-alt="Motte im Relais" width="100%"}
:::
:::{.column width="40%"}
Motte zwischen den Kontakten am Relais Nr. 70, Panel F, des Mark II Aiken Relay Calculator (Harvard University, 1947).[^motte]

:::
:::

[^edison]: Thomas Edison, Brief an William Orton, Präsident von Western Union, 13. März 1878.
[^mark2]: Grace Hopper, _The First Bug_, 1986.
[^motte]: Courtesy of the Naval Surface Warfare Center, Dahlgren, VA., 1988., Public Domain. [Link](https://de.wikipedia.org/wiki/Debuggen#/media/Datei:First_Computer_Bug,_1945.jpg)

## Von Bugs zu Debugging {auto-animate=true}

### Debugging ist nicht gleicht Troubleshooting

Der Begriff _Bug_ für technische Fehler lässt sich bis ins 19. Jahrhundert zurückverfolgen.

- 1878: Thomas Edison beschreibt „kleine Fehler“ in seinen Erfindungen als _Bugs_.[^edison]
- 1947: Mark II Aiken Relay Calculator – berühmter „Bug“ (Motte im Relais).[^mark2]

:::{.spacer-lg}
:::

::: {.callout-note appearance="simple" .fade-left}

> _Debugging bedeutet in der Regel herauszufinden, warum ein Entwurf nicht wie geplant funktioniert. Troubleshooting hingegen bedeutet herauszufinden, was an einer bestimmten Instanz eines Produkts defekt ist, wenn das Produktdesign an sich als korrekt gilt._ (nach Agans, 2002)[^agans]

<!-- ::: {.small-text}

> Debugging usually means figuring out why a design doesn't work as planned. Troubleshooting usually means figuring out what's broken in a particular copy of a product when the product's design is known to be good.(Agans, 2002)[^agans]

::: -->

:::

[^edison]: Thomas Edison, Brief an William Orton, Präsident von Western Union, 13. März 1878.
[^mark2]: Grace Hopper, _The First Bug_, 1986.
[^agans]: David, J. Agans. "The 9 indispensable rules for finding even the most elusive software and hardware problems." (2002).

## Vielfältigkeit eingebetteter Systeme

### von Sensoren bis zu Triebwerken

Eingebettete Systeme sind in ihrer Architektur, Funktionalität und Anwendung stark unterschiedlich.

- Standards und Vorschriften variieren deutlich je nach Branche
- Sicherheits- und Datenschutzanforderungen benötigen zusätzliche Entwicklung- und Testprozesse

:::{.columns}
:::{.column width="60%"}

![](Figuren/BeispieleEmbeddedSystems.png){fig-alt="Beispiele eingebetteter Systeme" width="100%"}
:::

<!--
:::{.column width="50%" .incremental .mid-space}

Einige Gemeinsamkeiten bestehen jedoch:

- Ressourcenbeschränkungen (Speicher, Rechenleistung)
- Echtzeitanforderungen
- Die gesamte Entwicklung erfordert **Zusammenarbeit zwischen verschiedenen Teams** und Disziplinen
 - **Besondere Schwierigkeit beim SW Debugging**
  - Eingeschränkte Beobachtbarkeit von internen Zuständen
  - Fehler treten oft nur unter spezifischen Echtzeitbedingungen auf
  - Hardwarezugriff ist begrenzt oder erfordert Spezialwerkzeuge

:::
-->

:::

::: {.slide-footnote}
Abbildung ist mit Adobe Firefly generiert.
:::

## Reproduzierbarkeit von Defekten

Ein zentrales Thema beim Debugging eingebetteter Software ist die _Reproduzierbarkeit_ von Fehlern.

::: {.mid-space}

- Effektives Debugging setzt voraus, dass ein Defekt **wiederholt ausgelöst** und sichtbar gemacht werden kann.

  - Von entscheidender Wichtigkeit: Fehler müssen **immer stimuliert** werden und **niemals nur simuliert**.

::: {.incremental}

- Vorteile der Reproduzierbarkeit:
  - Fehler lässt sich wieder sichtbar machen
  - Ursachenanalyse wird möglich
  - Fix kann überprüft werden

:::
:::

::: {.fragment}

In eingebetteten Systemen ist dies jedoch häufig schwierig wenn nicht unmöglich

- Fast immer sind mehrere Einflussfaktoren (z. B. zeitliche Randbedingungen, Umgebungsbedingungen) nicht unter unserer Kontrolle und oft auch nicht zuverlässig messbar.

:::

::: {.spacer-md}
:::

::: {.fragment}
:::{.callout-note appearance="simple"}

**Erfolgreiches Software‑Debugging setzt die Reproduzierbarkeit des Defekts voraus.**
:::
:::

# Technische und prozessuale Methoden {background-image="Figuren/First_Computer_Bug_1945.jpg" background-opacity="0.3" background-color="black"}

## Messung und Analyse zeitkritischer Abläufe {auto-animate=true}

### Beispiel: Elektromotorregler

Wir haben einen Regler für einen Elektromotor entwickelt.

- Die Funktion `computeControlRule` ist alle **15 ms** aufgerufen und hat ein **maximales Zeitbudget von 2 ms**.
- Wir müssen überprüfen wie lange unsere Funktion benötigt, um alle Berechnungen abzuschließen.

:::{.spacer-lg}
:::

::: {.fragment}
:::{data-id="timing_analysis"}

```c{code-line-numbers="|5"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    // Fehlerbehandlung und Übergabe der Regel
}
```

:::
:::

::: {.spacer-lg}
:::

:::{.fragment}

- **Wie können wir die Messungen durchführen?**

:::

<!--
## Instrumentierung und Tracing für Timing‑Analyse {auto-animate=true}

### Beispiel: Elektromotorregler

Wir haben einen Regler für einen Elektromotor entwickelt.

- Die Reglerfunktion ist alle **15 ms** aufgerufen und hat ein maximales Timing‑Budget von **2 ms**.
- Wir müssen überprüfen wie lange unsere Funktion wirklich braucht um alle die Berechnungen abzuschließen.

::: {.columns}
:::{.column width="90%"}

:::{.spacer-lg}
:::

:::{data-id="timing_analysis"}

**Ansatz 1: Pin-Toggling**

```c{code-line-numbers="5,7"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    gpio_set(GPIO_PIN_DEBUG, true);
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    gpio_set(GPIO_PIN_DEBUG, false);
    // Fehlerbehandlung und Übergabe der Regel
}
```

:::

:::
::: -->

## Messung und Analyse zeitkritischer Abläufe {auto-animate=true}

### Beispiel: Elektromotorregler

::: {.columns}
:::{.column width="55%"}

**Ansatz 1: Pin-Toggling**

:::{data-id="timing_analysis"}

```c{code-line-numbers="5,7"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    gpio_set(GPIO_PIN_DEBUG, true);
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    gpio_set(GPIO_PIN_DEBUG, false);
    // Fehlerbehandlung und Übergabe der Regel
}
```

:::

:::

:::{.column width="45%"}
::: {.fragment}
**Messungen:**
![](Figuren/Timing_Diagram.svg){width=100%}

::: {.small-text}

- Die gemessene Gesamtdauer beträgt jetzt ca. **1,8 ms**
- Sie liegt damit innerhalb des maximalen Time‑Budgets von 2 ms.

:::
:::

:::
:::

:::{.spacer-sm}
:::

::: {.fragment}

- Der Ansatz ermöglicht eine **höhere Genauigkeit** der Laufzeitmessung
  - Er hat eine **minimale Beeinflussung** der Echtzeit

:::

::: {.incremental}

- Der Ansatz hat aber auch Einschränkungen:

  - Nur begrenzte Ausgaben sind möglich.

:::

:::{.fragment}

- **Welche Alternativen bieten sich an?**

:::

<!--
## Instrumentierung und Tracing für Timing‑Analyse {auto-animate=true .center}

### Beispiel: Elektromotorregler

:::{data-id="timing_analysis"}

**Ansatz 2: printf**

```c{code-line-numbers="5,7"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    printf("applyControlRule - Start: %ld ms\n", get_time_ms());
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    printf("applyControlRule - End: %ld ms\n", get_time_ms());
    // Fehlerbehandlung und Übergabe der Regel
}
```

::: -->

## Messung und Analyse zeitkritischer Abläufe {auto-animate=true}

### Beispiel: Elektromotorregler

::: {.columns}
:::{.column width="55%"}
:::{data-id="timing_analysis"}

**Ansatz 2: printf**

```c{code-line-numbers="5,7"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    printf("applyControlRule - Start: %ld ms\n", get_time_ms());
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    printf("applyControlRule - End: %ld ms\n", get_time_ms());
    // Fehlerbehandlung und Übergabe der Regel
}
```

:::
:::

:::{.column width="45%"}

:::{.fragment}
**Ausgabe:**

```shell
applyControlRule - Start: 1000 ms
applyControlRule - End: 1006 ms
...
applyControlRule - Start: 1015 ms
applyControlRule - End: 1023 ms
...
```

::: {.small-text}

- Die gemessene Gesamtdauer beträgt jetzt ca. **6 ms!**

:::
:::
:::
:::

:::{.spacer-md}
:::

::: {.fragment}

- Jeder Aufruf von printf kann mehrere Millisekunden beanspruchen

  - Das **Zeitverhalten** wird dadurch erheblich beeinträchtigt.

  - Bessere Alternative sind dedizierte **dedizierte Logger‑Bibliotheken und Trace‑Tools**, z.B., Diagnostic Log and Trace in Automotive^[Diagnostic Log and Trace [DLT](https://www.autosar.org/fileadmin/standards/R20-11/CP/AUTOSAR_SWS_DiagnosticLogAndTrace.pdf)]

:::

## Präventive Maßnahmen {auto-animate=true .center}

### Eine Regelfunktion für die Warmwasserbereitung einer Kaffeemaschine

:::{data-id="defensive_programming"}

```cpp {code-line-numbers="|5"}
uint16_t computeHeatingTime(const uint8_t initialTemp,
                            const uint8_t finalTemp)
{
    const uint8_t conversion_factor = 1; // seconds per degree Celsius
    const uint8_t deltaT = finalTemp - initialTemp;
    const uint16_t time_sec = conversion_factor * deltaT;
    return time_sec;
}
```

:::

:::{.spacer-md}
:::

:::{.fragment}

- Was passiert, wenn `initalTemp=96` und `finalTemp=95`?

  :::{.fragment}

  - Bei `initialTemp=96` und `finalTemp=95` ergibt sich `time_sec=255`
  - Das entspricht **4 Minuten und 15 Sekunden!**

  :::

:::

:::{.spacer-md}
:::

::: {.callout-note appearance="simple" .fragment}
**Solche Fehler müssen bereits in der Entwicklungsphase entdeckt werden!**
:::

## Präventive Maßnahmen

### Ziel ist es, Fehler frühzeitig zu entdecken

:::{.spacer-md}
::::

::: {.mid-space}

- Prozessmaßnahmen:

  ::: {.fragment}

  - Code Reviews / Peer Reviews
  - Testautomatisierung (Continuous Integration / Continuous Deployment - CI/CD)
  - Testgetriebene Entwicklung (Test-Driven Development ; TDD)

  :::

:::

:::{.spacer-md}
::::

::: {.mid-space}

- Technische Maßnahmen:

  ::: {.fragment}

  - Statische Code-Analyse
  - Code‑Coverage‑Analyse (z. B. Line‑ und Branch‑Coverage)
  - Vertragsbasierte Programmierung (Design by Contract)

  :::

:::

## Präventive Maßnahmen {auto-animate=true}

::: {.spacer-sm}
:::

Eine mögliche Lösung, inspiriert von vertragsbasierter Programmierung (Design by Contract).

::: {.spacer-lg}
:::

:::{data-id="code_caffe_machine}

```cpp{code-line-numbers="|6-8"}
bool computeHeatingTime(const uint8_t initialTemp,
                        const uint8_t finalTemp,
                        uint16_t* heating_time)
{
    // Vorbedingungen prüfen
    if ((finalTemp < initialTemp) || (heating_time == NULL)) {
        return false;
    }

    const uint8_t conversion_factor = 1; // Sekunden pro Grad Celsius
    const uint8_t deltaT = finalTemp - initialTemp;
    const uint16_t time_sec = conversion_factor * deltaT;

    *heating_time = time_sec; // Wert schreiben
    return true;
}
```

:::

:::{.notes}

DbC beruht auf drei Bausteinen:

Vorbedingungen (Preconditions) → Bedingungen, die der Aufrufer erfüllen muss, bevor die Funktion korrekt arbeiten kann.

In deinem Code:

finalTemp >= initialTemp (sonst keine gültige Heizzeit).

heating_time != NULL (sonst kein Speicherziel).

→ Das ist der „Vertrag“: „Ich als Funktion rechne nur, wenn du mir gültige Eingaben gibst.“

Nachbedingungen (Postconditions) → Zusicherungen, die die Funktion garantiert, wenn sie erfolgreich zurückkehrt.

In deinem Code:

Wenn true zurückgegeben wird, ist garantiert, dass \*heating_time einen gültigen Wert enthält.

Wenn false zurückgegeben wird, bleibt der Speicher unverändert.

→ Das ist die Zusicherung: „Wenn ich Erfolg melde, bekommst du ein korrektes Ergebnis.“

Invarianten → Bedingungen, die während der gesamten Ausführung gültig bleiben.

Beispiel hier: conversion_factor ist konstant und positiv.

Damit bleibt die Berechnung stabil und nachvollziehbar.

:::

# Defektanalyse als Teil des Debugging‑Prozesses {background-image="Figuren/First_Computer_Bug_1945.jpg" background-opacity="0.3" background-color="black"}

## Defekte infolge von Kommunikations- und Interpretationsfehlern {auto-animate=true}

- Wir wollen ein Spurhaltewarnsystem (_Lane departure warning system_; LDWS) entwickeln.

:::{data-id="LDWS_Anf}

::: {.fragment}

- **Anforderung P.1:** LDWS muss aktiv sein, wenn die Kamera einen nominalen Zustand meldet (keine Degradation).
- **Anforderung P.2:** LDWS muss sich deaktivieren, wenn die Kamera eine Degradation meldet.

::: {.fragment}

- ...
  - ↳ **Kamera-Anforderung K.1:** Die Kamera muss eine Degradation melden, wenn ungeeignete Wetterbedingungen erkannt werden.
    - Beispiele: Regentropfen auf den Linsenelementen, Blendung durch Sonnenlicht

:::
:::
:::

::: {.slide-footnote}
Alle Anforderungen und Testfälle sind fiktiv und dienen nur zu Illustrationszwecken.
:::

::: {.notes}

- Besser Anforderung Formulierung für die Kamera:
  „Die Kamera muss eine Degradation melden, wenn äußere Bedingungen die Bildqualität so beeinträchtigen, dass definierte Funktions‑KPIs (z. B. Erkennungsrate, Spurtreue) unterschritten werden.“

:::

## Defekte infolge von Kommunikations- und Interpretationsfehlern {auto-animate=true}

:::{data-id="LDWS_Anf}

- **Anforderung P.1:** LDWS muss aktiv sein, wenn die Kamera einen nominalen Zustand meldet (keine Degradation).
- [**Anforderung P.2:** LDWS muss sich deaktivieren, wenn die Kamera eine Degradation meldet.]{style="color: dimgray;"}
- ...
  - [↳ **Kamera-Anforderung K.1:** Die Kamera muss eine Degradation melden, wenn ungeeignete Wetterbedingungen erkannt werden.]{style="color: dimgray;"}
    - [Beispiele: Regentropfen auf den Linsenelementen, Blendung durch Sonnenlicht]{style="color: dimgray;"}

:::
In einer späteren Testphase wird ein solcher Testreport erstellt:

::: {data-id="test-fall-example" .small-text}

| Test-ID      | Vorbedingungen                          | Soll-Verhalten | Ist-Verhalten                                                                                            | System-Versionierung             | Häufigkeit         | Kritikalität | Trace-Link |
| ------------ | --------------------------------------- | -------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------- | ------------------ | ------------ | ---------- |
| **P.1 - 01** | Ideales Wetter: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert sich jedoch in einer Stausituation. → **LDWS nicht aktiv!** | SW 1.1 / HW C / Net V2.0 / CFG A | **Reproduzierbar** | **Kritisch** | [Trace]()  |

<!--
| Test ID      | Vorbedingungen                            | Soll-Verhalten | Ist-Verhalten                                                                                                 | SW-Version | HW-Version |
| ------------ | ----------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------- | ---------- | ---------- |
| **P.1 - 01** | _Ideales Wetter_: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert sich jedoch sich in einer Stausituation. → **LDWS nicht aktiv!** | 1.1        | C          | -->

:::

::: {.slide-footnote}
Alle Anforderungen und Testfälle sind fiktiv und dienen nur zu Illustrationszwecken.
:::

## Defekte infolge von Kommunikations- und Interpretationsfehlern {auto-animate=true}

::: {data-id="test-fall-example" .tiny-text}

| Test-ID      | Vorbedingungen                          | Soll-Verhalten | Ist-Verhalten                                                                                            | System-Versionierung             | Häufigkeit     | Kritikalität | Trace-Link   |
| ------------ | --------------------------------------- | -------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------- | -------------- | ------------ | ------------ |
| **P.1 - 01** | Ideales Wetter: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert sich jedoch in einer Stausituation. → **LDWS nicht aktiv!** | SW 1.1 / HW C / Net V2.0 / CFG A | Reproduzierbar | Kritisch     | [DLT Link]() |

<!--
| Test ID      | Vorbedingungen                          | Soll-Verhalten | Ist-Verhalten                                                                                                 | SW-Version | HW-Version |
| ------------ | --------------------------------------- | -------------- | ------------------------------------------------------------------------------------------------------------- | ---------- | ---------- |
| **P.1 - 01** | Ideales Wetter: Tag, Sonne, kaum Wolken | LDWS ist aktiv | LDWS ist aktiv bei freier Fahrt, deaktiviert sich jedoch sich in einer Stausituation. → **LDWS nicht aktiv!** | 1.1        | C          | -->

:::

::: {.fragment .small-text}

Um die Kameravideos zu erhalten, mussten spezielle **Debug-Flags** in der Kamerasoftware aktiviert werden.

:::

::: {.columns .fragment}
:::{.column width="20%"}
![**Fahrerperspektive**](Figuren/Stau_Fahrerperspective.png){fig-alt="Fahrerperspektive: ideale Wetterbedingungen" width="90%"}

:::

:::{.column width="20%" .fragment}
![**Kameraperspektive**](Figuren/Stau_Kameraperspektive.png){fig-alt="Kameraperspektive: Blendung durch Sonnenlicht" width="90%"}

:::

:::{.column width="60%" .fragment .mid-space}

- **Für die Kamera sind diese Wetterbedingungen nicht geeignet**, auch wenn von der Fahrerperspektive, Wetterbedingungen ideal sind.
- Während die Anforderungen eindeutig den nominalen Kamerazustand (_keine Degradation_) beschrieben, wurden **die Testbedingungen indirekt über Wetterannahmen abgebildet**.

:::
:::

::: {.callout-note appearance="simple" .small-text .fragment}
In großen Projekten ist der erste Schritt der Defektanalyse die **Überprüfung, ob tatsächlich ein Defekt vorliegt!**.
:::

::: {.slide-footnote}
Alle Figuren sind mit Microsoft Copilot generiert.
:::

# Quiz {background-image="Figuren/First_Computer_Bug_1945.jpg" background-opacity="0.3" background-color="black"}

## Frage 1

::: {.spacer-lg}
:::

Welche Methode **beeinflusst** das Zeitverhalten einer Echtzeitfunktion stark?

::: {.spacer-lg}
:::

:::{.mid-space}

- **A.** Pin-Toggling.
- **B.** printf-Ausgaben.

:::

::: {.spacer-lg}
:::

::: {.callout-note appearance="simple" .fragment}
Richtige Antwort: **B**
:::

## Frage 2

::: {.spacer-lg}
:::

Wie sollten Defekte im Debugging behandelt werden?

::: {.spacer-lg}
:::

:::{.mid-space}

- **A.** Defekte werden **simuliert**, um sie nachzustellen.
- **B.** Defekte werden **stimuliert**, um sie reproduzierbar sichtbar zu machen.

:::

::: {.spacer-lg}
:::

::: {.callout-note appearance="simple" .fragment}
Richtige Antwort: **B** – Defekte müssen **stimuliert** werden, niemals nur simuliert.
:::

## Frage 3

::: {.spacer-lg}
:::

Welche präventive Maßnahme gehört zur Softwareentwicklung?

::: {.spacer-lg}
:::

:::{.mid-space}

- **A.** Statische Code-Analyse
- **B.** Debugging mit printf
- **C.** Hardware-Register auslesen

:::

::: {.spacer-lg}
:::

::: {.callout-note appearance="simple" .fragment}
Richtige Antwort: **A**
:::

# Backup

## Wenn man in die Tiefe des Mikrocontrollers schauen muss {auto-animate=true}

Manchmal ist es notwendig, Inhalte von Registern oder Speicher eines Mikrocontrollers auszulesen oder gezielt zu verändern.

:::{.incremental}

- Viele moderne **32‑Bit‑Mikrocontroller** besitzen dedizierte **On‑Chip Debug‑Interfaces**, z.B. Joint Test Action Group (_JTAG_) oder Serial Wire Debug (_SWD_)^[[https://www.jtag.com/de/](https://www.jtag.com/de/), [SWD für ARM](https://developer.arm.com/documentation/ihi0031/a/The-Serial-Wire-Debug-Port--SW-DP-/Introduction-to-the-ARM-Serial-Wire-Debug--SWD--protocol)]
  - Ermöglicht direkten Zugriff auf Register, Speicher und CPU‑Kontrolle

:::

::: {.spacer-sm}
:::

:::{data-id="jtag-figure"}
:::{.columns}
:::{.column width="50%"}
![Verbindung über JTAG^[Bildquelle: Erich Styger, „FTDI JTAG Connection“, MCU on Eclipse Blog, 2019. URL: [link](https://mcuoneclipse.com/wp-content/uploads/2019/10/ftdi-jtag-connection.png)]](Figuren/ftdi-jtag-connection_small.png){width="100%"}
:::

:::{.column width="50%"}
![Debugging mit GDB^[Bildquelle: Espressif Systems, „VS Code ESP-IDF Extension – GDB Commands“, offizielle Dokumentation. URL: [link](https://docs.espressif.com/projects/vscode-esp-idf-extension/en/latest/_images/gdb_commands.png)]](Figuren/gdb_commands_small.png){width="100%"}
:::
:::
:::

::: {.notes}
ARM = Advanced RISC Machine
RISC= Reduced Instruction Set Computer

- Typische Werkzeuge:
  - **JTAG (Joint Test Action Group)**
  - **SWD (Serial Wire Debug)** – abgespeckte Variante für ARM Cortex‑M
  - **OpenOCD (Open On‑Chip Debugger)** + **GDB (GNU Debugger)** im Open‑Source‑Umfeld
  - Kommerzielle Alternativen: **Segger J‑Link**, **Lauterbach TRACE32**, Hersteller‑IDEs (z. B. STM32CubeIDE, NXP MCUXpresso)
- On‑Chip Debug-Interfaces erlauben Daten auszulesen, Speicher und Register zu verändern und der Programmfluss zu beeinflussen
  - Breakpoints erlauben es, den Code gezielt anzuhalten und zu überwachen

:::

## Wenn man in die Tiefe des Mikrocontrollers schauen muss {auto-animate=true}

:::{data-id="jtag-figure"}
:::{.columns}
:::{.column width="50%"}
![Verbindung über JTAG^[Bildquelle: Erich Styger, „FTDI JTAG Connection“, MCU on Eclipse Blog, 2019. URL: [link](https://mcuoneclipse.com/wp-content/uploads/2019/10/ftdi-jtag-connection.png)]](Figuren/ftdi-jtag-connection_small.png){width="100%"}
:::

:::{.column width="50%"}
![Debugging mit GDB^[Bildquelle: Espressif Systems, „VS Code ESP-IDF Extension – GDB Commands“, offizielle Dokumentation. URL: [link](https://docs.espressif.com/projects/vscode-esp-idf-extension/en/latest/_images/gdb_commands.png)]](Figuren/gdb_commands_small.png){width="100%"}
:::
:::
:::

::: {.space-xl}
:::

:::{.mid-space}

- Debugging erfordert oft die Verwendung spezieller **Debug-Flags**
  - Diese deaktivieren Optimierungen und verändern Ablauf sowie Performance gegenüber optimierten Build

:::

::: {.space-lg}
:::

::: {.space-lg}
:::

:::{.fragment}

:::{.callout-note appearance="simple"}

**Zeitliche Probleme bleiben im Debug‑Build häufig verborgen**, da die reale Code-Performance im optimierten Build abweicht.

:::
:::

## Das V‑Modell {auto-animate=true}

### Anforderungen, Tests und Kommunikationsrisiken zwischen Teams

- Das V‑Modell entstand Ende der 1970er Jahre als Erweiterung des Wasserfallmodells,
  vorgeschlagen von Barry W. Boehm (1979), um Verifikation und Validierung systematisch zu verankern.

::: {.r-stack}
![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell_Teil1.svg){.fragment width="100%"}

![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell_Teil2.svg){.fragment width="100%"}

![Das V‑Modell – eine vereinfachte Darstellung](Figuren/V_modell.svg){.fragment width="100%"}

:::

::: {.slide-footnote}
Boehm, B. W. (1979). _Guidelines for Verifying and Validating Software Requirements and Design Specifications._ In Proceedings of the European Conference on Applied Software Engineering
:::

:::{.notes}

Barry W. Boehm

- Geboren 1935, gestorben 2022.
- Professor an der University of Southern California (USC).
- Arbeitete zuvor u. a. bei RAND Corporation und TRW.
- Bekannt für grundlegende Beiträge zur Softwareentwicklung, u. a.:
- Spiralmodell (1986) – ein iteratives Vorgehensmodell.
- COCOMO‑Modell (Constructive Cost Model) – zur Aufwandsschätzung von Softwareprojekten.
- V‑Modell (1979) – als Erweiterung des Wasserfallmodells.

:::

## Das V‑Modell {auto-animate=true}

### Anforderungen, Tests und Kommunikationsrisiken zwischen Teams

::: {.spacer-sm}
:::

::: {.columns}
:::{.column width="45%"}
![](Figuren/V_modell.svg){fig-alt="V-Model without axes" width="100%"}
:::

::: {.column width="55%" .small-text}
::: {.spacer-sm}
:::

- Anforderungen werden iterativ verfeinert, Testfälle daraus abgeleitet
- Anforderungen definieren das **Soll‑Verhalten**, Testfälle prüfen es
- In der Testphase wird das Soll‑Verhalten ausschließlich durch die Testfälle beschrieben

::: {.spacer-sm}
:::

::: {.fragment fragment-index=2}

- **Unterschiedliche Teams** arbeiten häufig auf verschiedenen Ebenen des V‑Modells
- Entwurfsteams sind typischerweise nicht identisch mit den Teams, die Tests und Verifikation durchführen
- **Kommunikationsfehler** können dazu führen, dass Anforderungen oder Testfälle falsch abgeleitet werden
- Solche Fehler werden oft erst in der Testphase erkannt

:::
:::
:::

:::{.spacer-sm}
:::

::: {.fragment}

- Software‑Debugging erfolgt häufig als Reaktion auf einen Defekt.
- Defekte entstehen in der Test- und Validierungsphase, wenn das **Ist-Verhalten** nicht das **Soll-Verhalten** entspricht.

:::

<!-- # To be removed

## Find a title here

Debugging ist daher ein notwendiger Schritt, um Tests zu validieren, Defekte zu verstehen und das System nachhaltig zu verbessern.“

- Debugging vs. Testing – Unterschied und Rolle im Entwicklungsprozess
- Kosten von Fehlern – warum Prävention wichtig ist
- Verifikation des Defekts – Testreport und Reproduzierbarkeit
- System verstehen – HW, SW, Timing als mögliche Ursachen
- Debugging-Werkzeuge
  - JTAG, UART
  - Trace & Logging (DLT, RTEdbg, SystemView)
  - Debug-Build vs. Optimized Build
- Timing-Probleme – die schwierigsten Fehler
- Prävention statt Heilung
  - Test Driven Development (TDD)
  - Code Coverage
  - Static Code Analysis
- Fehlerbehebung & Absicherung – Regressionstests, Anforderungen ergänzen
- Dokumentation & Lessons Learned

## Debugging und Kommunikationsfehler in Embedded Software

### Anzahl der beteiligten Gruppen erschwert die Defektanalyse

:::{.spacer-sm}
:::

::: {.columns}
:::{.column width="33%"}
![**Thermostat (Smart Home)**](Figuren/Thermostat_square.png){fig-alt="Heizkörper-Thermostat" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** <3
- **Komplexität:** Niedrig

:::
:::

:::{.column width="33%"}
![**Vollautomatik Kaffeemaschine**](Figuren/KaffeMachine_square.png){fig-alt="Vollautomatik Kaffeemaschine" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** 5-10
- **Komplexität:** Mittel

:::
:::

:::{.column width="33%"}
![**Fahrassistenzsysteme**](Figuren/Auto_square.png){fig-alt="Fahrerassistenzsysteme" width="100%"}

::: {.small-text .fragment}

- **Geschätzte Gruppen:** >20
- **Komplexität:** Hoch

:::
:::

:::

::: {.fragment .callout-note appearance="simple" .small-text}
**Risiko von Kommunikationsfehler wächst mit der Zahl der beteiligten Gruppen.**
:::

::: {.slide-footnote}
Alle Figuren sind mit Microsoft Copilot generiert.
:::

## Die Soll-Funktion ergibt sich aus den Anforderungen

### Die Testfällen beschrieben aber sie explicit

::: {.columns}
:::{.column width="70%"}


:::

::: {.column width="30%" .mid-space}

- Jede Ebene ableitet detailliertere Anforderungen für die nächste Ebene sowie passende _Testfälle_.
- Anforderungen beschrieben das Soll-Verhalten für mehrere Input Bedingungen.
- Während eine Testphase, ist die Soll-Verhalten nur anhand von einem Test case für das _System Unter Test_ (SUT) spezifiziert.
- Test cases spezifizieren das Soll-Verhalten für ein spezifische, sehr detaillierten Fall

:::
:::

::: {.callout-note appearance="simple" .small-text}
Fehler können sowohl in der Spezifikation der Anforderungen als auch in der Ableitung der Testfälle entstehen.
:::

::: {.slide-footnote}
Bild von Michael Pätzold, S. Seyfert, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/de/).
:::

## Add me

- Jede System muss sein Input monitorieren und off-line analysieren können
- Logging und Tracing sind kritisch
- Werkzeuge und Techniken um das System Verhalten zu beobachten
- Beispiel: Monitoring von Kaffee Temperatur in einer Kaffeemaschine
- Wie können wir das Verhalten beobachten ohne das System zu beeinflussen?
  - Nie printf befehle in Embedded Systems benutzen!!
  - Log Dateien mit Zeitstempel und Kontextinformationen
  - Trace Analyse Werkzeuge
  - Autmotive ist DLT (Diagnostic Log and Trace) Standard
- Hardware Debugging Werkzeuge
  - JTAG, SWD, etc..
  - Logic Analyzer
  - In-Circuit Emulatoren
- Software Debugging Werkzeuge
  - GDB, LLDB, etc..
  - IDE integrierte Debugger
  - Spezielle Debugging Werkzeuge für Embedded Systems
- Debugging Techniken
  - Breakpoints und Watchpoints
  - Step-by-Step Ausführung
  - Speicher- und Register-Inspektion
  - Code Coverage Analyse
  - Performance Profiling
  - Timing Analyse
  - Kommunikationsprotokoll Analyse
- Zusammenarbeit im Team

  - Gemeinsame Debugging Sitzungen

  - Nie ein Fehler simulieren, Die Fehler müssen stimuliert werden!!
  - Problem: Nicht alle Fehler sind reproduzierbar da nicht alle Input Bedingungen kontrollierbar sind und Status Informationen fehlen können
  - Beispiel: Intermittierende Fehler durch elektromagnetische Störungen
  - Debugging ist ein iterativer Prozess
  - Hypothese generieren, testen, verfeinern
  - Geduld und Ausdauer sind entscheidend

## Instrumentierung und Tracing für Timing‑Analyse - Details

### Beispiel: Elektromotorregler

::: {.columns}
:::{.column width="60%"}

**Besser Ansatz: Pin-Toggling**

```c{code-line-numbers="5,7"}
bool applyControlRule(const InputParameter* motor_state)
{
    bool is_rule_successfully_computed = false;
    ControlRule control_rule;
    gpio_set(GPIO_PIN_DEBUG, true);
    is_rule_successfully_computed = computeControlRule(motor_state, &control_rule);
    gpio_set(GPIO_PIN_DEBUG, false);
    // Fehlerbehandlung und Übergabe der Regel
}
```

:::

:::{.column width="40%"}
**Messungen:**
![](Figuren/Timing_Diagram.svg){width=100%}

::: {.small-text}
Die gemessene Gesamtdauer beträgt jetzt ca. **1,8 ms** und liegt damit innerhalb des maximalen Time‑Budgets von 2 ms.
:::

:::
:::

:::{.spacer-sm}
:::

::: {.fragment .small-text}

- Diese Art von zeitlichen Messungen ermöglicht im Vergleich zum printf‑Ansatz:

  - **Deutlich höhere Genauigkeit** der Laufzeitmessung
  - **Reproduzierbarkeit** der Ergebnisse über viele Zyklen hinweg
  - **Minimale Beeinflussung** der Echtzeit (Overhead im Nanosekunden-/Mikrosekundenbereich)

:::

::: {.fragment .small-text}
Der Ansatz hat aber auch Einschränkungen:

- Nur begrenzte Ausgaben sind möglich. Vorgeschlagene Alternative: **Logger und Trace‑Tools**
  - z.B., Diagnostic Log and Trace in Automotive[^Diagnostic Log and Trace [DLT](https://www.autosar.org/fileadmin/standards/R20-11/CP/AUTOSAR_SWS_DiagnosticLogAndTrace.pdf)]

:::
--->
